{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Principal Component Analysis\n",
    "\n",
    "This notebook is a showcase of our `pca` module, which implements Principal Component Analysis (PCA).\n",
    "\n",
    "PCA is a method for dimensionality reduction, performing well on affine-linear data.\n",
    "\n",
    "First, the data is centered by subtracting the average datapoint from each datapoint. Then, we perform singular value decomposition on the centered data.\n",
    "\n",
    "The trick is to reverse the singular value decomposition by setting the least important singular values to 0, and then multiplying the matrices back together. This is possible because the singular values are ordered by magnitude, and the least important singular values are the ones that contribute the least to the variance in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import init_notebook\n",
    "from test_module import test_function\n",
    "from pca import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Helper module for visualization in all notebooks\n",
    "import notebook_utils as utils\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "\n",
    "test_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis\n",
    "We load the data from a file and perform PCA.\n",
    "\n",
    "It's apparent that one of the principal components accounts for over 99.3% of the variance in the data, which motivates us to the assumption that the other principal component is just noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pca_dataset.txt\", 'r') as file:\n",
    "    # Load data inta a Python ndarray, shape (100, 2)\n",
    "    global data_matrix\n",
    "    data_matrix = np.loadtxt(file, delimiter=' ')\n",
    "    assert data_matrix.shape == (100, 2)\n",
    "\n",
    "pca_result = PCA.pca(data_matrix)\n",
    "\n",
    "# This is how we can access the data of our PCA result\n",
    "U, S, Vh, mean = pca_result\n",
    "E = pca_result.energy\n",
    "\n",
    "print(f'Energies: {E}\\nSingular values ordered by magnitude: {S}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reversing PSA to verify correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix_reconstructed = pca_result.reverse_pca(2)\n",
    "\n",
    "# Before and after display side-by-side\n",
    "display_side_by_side = False\n",
    "if display_side_by_side:\n",
    "    for o, r in zip(data_matrix, data_matrix_reconstructed):\n",
    "        print(f'o: {o}, r: {r}')\n",
    "\n",
    "assert np.allclose(data_matrix, data_matrix_reconstructed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting data\n",
    "\n",
    "The plot looks suspiciously linear, which further supports the assumption that a linear model is very suitable for this dataset. The green principal component seems to be just noise, with no discernable patterns. The points align strongly with the red principal component.\n",
    "\n",
    "PSA is a great approach for data that shows affine-linear behavior, as opposed to data on curved manifolds. This is why it makes sense to use PSA for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_data_with_pcs(data_matrix, Vh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate 1D\n",
    "\n",
    "Eliminating the lesser principal component and approximating the data with only the first principal component, the data is approximated to a 1D line. This is done by simply setting the second singular value to 0 inside the matrix of singular values given by `S`, and then reversing the PCA by multiplying the matrices `U`, `S`, and `Vh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supply 1 to reverse_pca to approximate the data to 1D\n",
    "approximated_data = pca_result.reverse_pca(1)\n",
    "\n",
    "utils.plot_data_with_pcs(approximated_data, pca_result.Vh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image\n",
    "\n",
    "We load the racoon image in gray and perform PCA on it.\n",
    "\n",
    "**Note:** `scipy.misc.face` from the exercise sheet is deprecated because `scipy.misc` is deprecated. We use `scipy.datasets.face` instead.\n",
    "\n",
    "Scaling the image to 249x185 pixels, we know that the image has **185 principal components.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image as `ndarray`\n",
    "my_image = sc.datasets.face(gray=True)\n",
    "my_image = utils.rescale_greyscale_img(my_image, 249, 185)\n",
    "\n",
    "plt.imshow(my_image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA on image\n",
    "We perform PCA on the image, and print some of the singular values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the columns as datapoints, hence specify the flag. This 'flattens' the image\n",
    "# The flag also ensures that the reconstructed image is transposed back to the original shape\n",
    "pca_result_img = PCA.pca(my_image, treat_columns_as_datapoints=True)\n",
    "\n",
    "utils.print_pca_info(pca_result_img, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of reconstructions\n",
    "\n",
    "We visualize for different numbers of components.\n",
    "\n",
    "Slight quality detriments are noticeable starting from 50 components, especially around the racoon's fur.\n",
    "\n",
    "The image with 10 components is barely recogniziable despite preserving over 83% of the energy. The racoon's face is still possible to identify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize reconstructions with different numbers of principal components\n",
    "for num_components in [pca_result_img.S.shape[0], 120, 50, 10]:\n",
    "    utils.plot_reconstructed_image(pca_result_img, num_components, my_image.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy loss\n",
    "To lose less than 1% of the energy, we need to preserve 76 principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of components needed to retain a percentage of the energy\n",
    "energy_threshold = 0.99\n",
    "n = pca_result_img.min_components_until(energy_threshold)\n",
    "\n",
    "print(f'Number of components needed to retain 99% of the energy: {n}')\n",
    "print(f'We can remove {pca_result_img.S.shape[0] - n} dimensions from our image\\'s columns while retaining {energy_threshold*100}% of the information')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Vadere Trajectory Analysis\n",
    "\n",
    "Given a dataset of 15 pedestrians with 1000 2D coordinates each, we load the data to perform PCA on it.\n",
    "\n",
    "First, we visualize the data for the **first two pedestrians:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data_DMAP_PCA_vadere.txt\", 'r') as file:\n",
    "    # Load data inta a Python ndarray, shape (100, 2)\n",
    "    global trajectory_matrix\n",
    "    trajectory_matrix = np.loadtxt(file, delimiter=' ')\n",
    "    assert trajectory_matrix.shape == (1000, 30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3.1: Visualizing the path of the ﬁrst two pedestrians in the two-dimensional space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_pedestrian_figure_variant1(trajectory_matrix)\n",
    "utils.plot_pedestrian_figure_variant2(trajectory_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations\n",
    "\n",
    "- Each trajectory starts and ends at the starting point\n",
    "- The pedestrians appear to be walking in circles\n",
    "- There seems to be a lot of overlap\n",
    "- The pedestrians are moving in thin rings. Perhaps PCA can thin out the rings while preserving the overall shape of the trajectories, leading to a high-energy preservation of the data even with a low number of principal components.\n",
    "- This is a hypothesis that we can test out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3.1: Visualizing the path of the ﬁrst two pedestrians in the two-dimensional space: Gradient Coloring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data represents 15 pedestrians with coordinates (x, y). The columns are the timesteps\n",
    "\n",
    "We visualize the points for a given timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_result = PCA.pca(trajectory_matrix)\n",
    "\n",
    "utils.print_pca_info(pca_result, 3)\n",
    "\n",
    "utils.reconstruct_and_plot_trajectory(pca_result, 2)\n",
    "utils.reconstruct_and_plot_trajectory(pca_result, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations\n",
    "\n",
    "- Two principal components accurately reconstruct the vague shape of the trajectory.\n",
    "- However, drawing the velocity arrows from the original data, it is apparent that the velocity arrows deviate slightly from the original trajectory.\n",
    "- Three principal components are much better at imitating the original shape of the trajectory.\n",
    "\n",
    "Overall, it is impressive that the PCA is able to reconstruct the seemingly non-linear trajectory very accurately with only 3 principal components, given that it is a 90% reduction in dimensionality. We hypothesize that this is because the trajectories have many points that are very close to each other, allowing the PCA to act as a locally linear approximator. Even the rounded corners are well approximated by the PCA, which can be explained by the fact that they contain lots of data points. \n",
    "\n",
    "We might want to try PCA with less than 1000 data points, to see if the PCA is able to reconstruct the trajectory with less data points. The data would still have 30 principal components, but the PCA would have less data points to work with, possibly allowing for a more equal distribution of the energies. This would be a good test of the PCA's ability to approximate the trajectory with less data points."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
